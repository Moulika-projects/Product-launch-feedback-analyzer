# -*- coding: utf-8 -*-
"""app4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1k6HyL2WqV_d-bckXRTiqjU12uVS5rhyQ
"""

from google.colab import files
uploaded = files.upload()

import pandas as pd

# Load dataset
df = pd.read_csv("flipkart_product.csv", encoding="ISO-8859-1")

# Reduce dataset to 50,000 rows (adjust number as needed)
df_sampled = df.sample(n=58000, random_state=42)

# Save new dataset
df_sampled.to_csv("flipkart_product_sampled.csv", index=False)

print("New dataset size:", df_sampled.shape)

import pandas as pd
import re
import numpy as np

# Load dataset
file_path = "flipkart_product_sampled.csv"
df = pd.read_csv(file_path, encoding="ISO-8859-1")

# Drop rows with missing reviews
df = df.dropna(subset=['Review'])

# Fix for SettingWithCopyWarning when modifying 'Price'
df.loc[:, 'Price'] = df['Price'].astype(str).str.replace(r'\D', '', regex=True)
df['Price'] = pd.to_numeric(df['Price'], errors='coerce')

# Fix for SettingWithCopyWarning when modifying 'Rate'
df.loc[:, 'Rate'] = pd.to_numeric(df['Rate'], errors='coerce')
# Display cleaned data
df.head()

print(df.isnull().sum())

df['Rate'] = df['Rate'].astype(float)  # Ensure 'Rate' is numeric
df['Rate'] = df['Rate'].fillna(df['Rate'].median())  # Fill missing values with median
df['Summary'] = df['Summary'].astype(str).fillna('No Summary')

print(df.isnull().sum())

import re
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer

nltk.download('stopwords')
nltk.download('punkt')
nltk.download('wordnet')
nltk.download('punkt_tab')

# Initialize lemmatizer
lemmatizer = WordNetLemmatizer()
stop_words = set(stopwords.words('english'))

def preprocess_text(text):
    if isinstance(text, str):  # Ensure text is a string
        text = text.lower()  # Convert to lowercase
        text = re.sub(r'[^\w\s]', '', text)  # Remove punctuation
        words = word_tokenize(text)  # Tokenization
        words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]  # Lemmatization & stopword removal
        return ' '.join(words)
    return text

# Apply preprocessing to 'Review' and 'Summary' columns
df['Review'] = df['Review'].apply(preprocess_text)
df['Summary'] = df['Summary'].apply(preprocess_text)

print(df.head())  # Display the cleaned data

import pandas as pd
import torch
from transformers import BertTokenizer, BertForSequenceClassification
from torch.utils.data import DataLoader, Dataset
import torch.nn.functional as F
from sklearn.model_selection import train_test_split
import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
import re


# Load dataset
df = pd.read_csv("flipkart_product_sampled.csv", encoding="ISO-8859-1")  # Update with your file name
df = df.dropna(subset=["Summary"])  # Remove empty reviews

import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
import string


def preprocess_text(text):
    if isinstance(text, float):  # Check if text is a float (NaN or missing values)
        text = ""  # Replace with an empty string or a placeholder like "No Summary"
    text = text.lower()  # Convert to lowercase
    tokens = word_tokenize(text)  # Tokenization
    tokens = [word for word in tokens if word.isalnum()]  # Remove punctuation
    tokens = [word for word in tokens if word not in stopwords.words("english")]  # Remove stopwords
    return " ".join(tokens)

# Apply preprocessing to the review column
df["Cleaned_Summary"] = df["Summary"].astype(str).apply(preprocess_text)

# Display the cleaned data
print(df[["Summary", "Cleaned_Summary"]].head())

import pandas as pd
import torch
from transformers import BertTokenizer, BertForSequenceClassification
from torch.utils.data import DataLoader, Dataset
import torch.nn.functional as F
from sklearn.model_selection import train_test_split
import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
import re

import pandas as pd
import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from sklearn.model_selection import train_test_split

# Ensure NLTK dependencies are downloaded
nltk.download("stopwords")
nltk.download("punkt")

# Load dataset
df = pd.read_csv("flipkart_product_sampled.csv", encoding="ISO-8859-1")
df = df.dropna(subset=["Summary"])  # Remove empty reviews

# Define text preprocessing function
def preprocess_text(text):
    if not isinstance(text, str):  # Check if the value is not a string
        text = ""  # Replace NaN or non-string values with an empty string
    text = text.lower()  # Convert to lowercase
    tokens = word_tokenize(text)  # Tokenization
    tokens = [word for word in tokens if word.isalnum()]  # Remove punctuation
    tokens = [word for word in tokens if word not in stopwords.words("english")]  # Remove stopwords
    return " ".join(tokens)

# Apply preprocessing
df["Cleaned_Summary"] = df["Summary"].astype(str).apply(preprocess_text)

# Fix the "Rate" column issue
df.loc[:, "Rate"] = pd.to_numeric(df["Rate"], errors="coerce")  # Convert safely
df = df.dropna(subset=["Rate"]).copy()                          # Remove invalid and make a clean copy
df.loc[:, "Rate"] = df["Rate"].astype(int)                      # Convert to integer

# Assign sentiment labels
df.loc[:, "Sentiment"] = df["Rate"].apply(lambda x: 2 if x > 3 else (1 if x == 3 else 0))

# Split into train & test
train_texts, val_texts, train_labels, val_labels = train_test_split(
    df["Cleaned_Summary"].tolist(), df["Sentiment"].tolist(), test_size=0.2, random_state=42
)

# Convert labels to integers if needed
y_train_cleaned = [int(label) for label in y_train]

# Now compute class weights
from sklearn.utils.class_weight import compute_class_weight
import numpy as np
import torch

classes = np.unique(y_train_cleaned)
class_weights = compute_class_weight(
    class_weight='balanced',
    classes=classes,
    y=y_train_cleaned
)
class_weights = torch.tensor(class_weights, dtype=torch.float)

from transformers import Trainer

import torch.nn as nn

# Define a custom Trainer that injects class weights into the loss
class WeightedTrainer(Trainer):
    def compute_loss(self, model, inputs, return_outputs=False):
        labels = inputs.get("labels")
        outputs = model(**inputs)
        logits = outputs.get("logits")

        loss_fct = nn.CrossEntropyLoss(weight=class_weights.to(model.device))
        loss = loss_fct(logits, labels)

        return (loss, outputs) if return_outputs else loss

df["Rate"] = df["Rate"].astype(int)

print(df["Rate"].unique())

import torch
from torch.utils.data import Dataset, DataLoader
from transformers import BertTokenizer

# Tokenizer Initialization
tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")

# Tokenize and convert to input tensors
train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=512)
val_encodings = tokenizer(val_texts, truncation=True, padding=True, max_length=512)

# Define Dataset class
class SentimentDataset(Dataset):
    def __init__(self, encodings, labels):
        self.encodings = {key: torch.tensor(val) for key, val in encodings.items()}
        self.labels = torch.tensor(labels)

    def __len__(self):
        return len(self.labels)

    def __getitem__(self, idx):
        item = {key: val[idx] for key, val in self.encodings.items()}
        label = self.labels[idx]
        return item, label

# Ensure labels are lists
train_labels = list(train_labels)
val_labels = list(val_labels)

# Create dataset instances
train_dataset = SentimentDataset(train_encodings, train_labels)
val_dataset = SentimentDataset(val_encodings, val_labels)

# Create DataLoaders
train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False)

model = BertForSequenceClassification.from_pretrained("bert-base-uncased", num_labels=3)  # 3 classes: pos, neu, neg
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)
optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)

epochs = 5
for epoch in range(epochs):
    model.train()
    total_loss = 0
    print(f"Starting Epoch {epoch+1}")  # Debugging

    for batch_idx, batch in enumerate(train_loader):
        print(f"Processing batch {batch_idx+1}/{len(train_loader)}")  # Debugging

        inputs, labels = batch
        inputs = {key: val.to(device) for key, val in inputs.items()}
        labels = labels.to(device).long()  # Ensure labels are correct

        optimizer.zero_grad()
        outputs = model(**inputs, labels=labels)
        loss = outputs.loss
        loss.backward()
        optimizer.step()
        total_loss += loss.item()

    print(f"Completed Epoch {epoch+1}, Loss: {total_loss:.4f}")

# Save fine-tuned model
model.save_pretrained("fine_tuned_bert_sentiment")
tokenizer.save_pretrained("fine_tuned_bert_sentiment")

from sklearn.metrics import accuracy_score

model.eval()  # Set model to evaluation model
preds, true_labels = [], []

with torch.no_grad():  # Disable gradient computation for efficiency
    for batch in val_loader:
        inputs, labels = batch
        inputs = {key: val.to(device) for key, val in inputs.items()}
        labels = labels.to(device)  # No need to re-convert to tensor

        outputs = model(**inputs)  # Forward pass
        preds.extend(torch.argmax(outputs.logits, dim=1).cpu().numpy())  # Get predicted labels
        true_labels.extend(labels.cpu().numpy())  # Get actual labels

# Calculate accuracy
accuracy = accuracy_score(true_labels, preds)
print(f"Validation Accuracy: {accuracy:.4f}")

from sklearn.metrics import classification_report

print(classification_report(true_labels, preds))

from sklearn.metrics import confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

cm = confusion_matrix(true_labels, preds)
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues")
plt.xlabel("Predicted Label")
plt.ylabel("True Label")
plt.show()

from transformers import BertTokenizer, BertForSequenceClassification

model_path = "./fine_tuned_bert_sentiment"
tokenizer = BertTokenizer.from_pretrained(model_path)
model = BertForSequenceClassification.from_pretrained(model_path)
model.eval()

!zip -r fine_tuned_bert_sentiment.zip fine_tuned_bert_sentiment
from google.colab import files
files.download("fine_tuned_bert_sentiment.zip")

import torch
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

def predict_sentiment(text):
    inputs = tokenizer(text, return_tensors="pt", truncation=True, padding=True, max_length=128).to(device)
    with torch.no_grad():
        outputs = model(**inputs)
    logits = outputs.logits
    pred = torch.argmax(logits, dim=1).item()
    # Map numeric label to sentiment
    label_map = {0: "Negative", 1: "Neutral", 2: "Positive"}
    return label_map[pred]

test_reviews = [
    "This product is amazing!",
    "Absolutely terrible, I hate it.",
    "The price is too high, not worth it.",
    "Good quality and very durable.",
    "Cheap price but bad quality."
]

for review in test_reviews:
    sentiment = predict_sentiment(review)
    print(f"Review: {review}\nPredicted Sentiment: {sentiment}\n")

import torch
import spacy
from transformers import pipeline

# Load SpaCy for aspect extraction
nlp = spacy.load("en_core_web_sm")

# Load pre-trained sentiment analysis model
aspect_sentiment_model = pipeline("sentiment-analysis")

# Set up device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

# Function to predict overall sentiment
def predict_sentiment(text):
    inputs = tokenizer(text, return_tensors="pt", truncation=True, padding=True, max_length=128).to(device)
    with torch.no_grad():
        outputs = model(**inputs)
    logits = outputs.logits
    pred = torch.argmax(logits, dim=1).item()

    label_map = {0: "Negative", 1: "Neutral", 2: "Positive"}
    return label_map[pred]

# Extract aspects (Price & Quality)
def extract_aspects(text):
    doc = nlp(text)
    aspects = {"Price": "Not Mentioned", "Quality": "Not Mentioned"}

    for token in doc:
        if token.head.text.lower() in ["price", "quality"]:
            aspect_type = "Price" if token.head.text.lower() == "price" else "Quality"
            aspects[aspect_type] = token.text

    return aspects

# Predict aspect sentiment and generate recommendations
def predict_aspect_sentiment(text):
    aspects = extract_aspects(text)
    aspect_sentiments = {"Price": "Not Mentioned", "Quality": "Not Mentioned"}
    recommendations = []  # Store recommendations

    for aspect in aspects.keys():
        if aspects[aspect] != "Not Mentioned":  # Only predict if mentioned
            result = aspect_sentiment_model(text)
            sentiment_label = result[0]['label']

            if "NEGATIVE" in sentiment_label:
                aspect_sentiments[aspect] = "Negative"
            elif "POSITIVE" in sentiment_label:
                aspect_sentiments[aspect] = "Positive"
            else:
                aspect_sentiments[aspect] = "Neutral"

            if "cheap price" in text.lower():
                aspect_sentiments["Price"] = "Positive"

            # Generate recommendations
            if aspect == "Price":
                if aspect_sentiments["Price"] == "Negative":
                    recommendations.append("Consider offering discounts or special offers to attract customers.")
                elif aspect_sentiments["Price"] == "Positive":
                    recommendations.append("Maintain competitive pricing to retain positive customer perception.")

            if aspect == "Quality":
                if aspect_sentiments["Quality"] == "Negative":
                    recommendations.append("Improve product quality by addressing durability and performance issues.")
                elif aspect_sentiments["Quality"] == "Positive":
                    recommendations.append("Continue maintaining high-quality standards to satisfy customers.")

    return aspect_sentiments, recommendations

# Test reviews
test_reviews = [
    "The price is too high, not worth it.",
    "Good quality and very durable.",
    "Cheap price but bad quality.",
    "I love this product!",
    "The design is nice, but it's too expensive."
]

# Run sentiment analysis for all reviews
for review in test_reviews:
    overall_sentiment = predict_sentiment(review)
    aspect_sentiments, recommendations = predict_aspect_sentiment(review)

    print(f"Review: {review}")
    print(f"Overall Sentiment: {overall_sentiment}")
    print(f"Aspect-Based Sentiment: {aspect_sentiments}")

    if recommendations:
        print("Recommendations:")
        for rec in recommendations:
            print(f"- {rec}")

    print("\n")

